{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests, requests_cache, json, pandas as pd, numpy as np\n",
    "requests_cache.install_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                    BANGOR\n",
       "1     BIRMINGHAM (ANN TUSC)\n",
       "2       BOSTON (MANCHESTER)\n",
       "3    BURLINGTON-PLATTSBURGH\n",
       "4                 CHARLOTTE\n",
       "Name: FCC DMA, dtype: object"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Importing Super Tuesday DMAs file\n",
    "dma_list = pd.read_excel(r'Super Tuesday DMAs.xlsx')\n",
    "dma_list_1 = dma_list[dma_list['SuperTues'] == 1]['FCC DMA'].str.replace('\\xa0', ' ')\n",
    "super_tuesday_dma = dma_list_1.str.slice(stop =-7).str.strip()\n",
    "super_tuesday_dma[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import date\n",
    "## Given a boundary date_1 in mm/dd/yy format, returns a filter for \n",
    "## political files created between then and the current date.\n",
    "def opif_timestamp_to_present(date_1):\n",
    "    date_2 = date.today().strftime(\"%m/%d/%y\")\n",
    "    timestamp = {\"create_ts\": str(date_1) + ' - ' + date_2}\n",
    "    return timestamp\n",
    "\n",
    "## Returns a filter for political files between date_1 and date_2.\n",
    "def opif_timestamp_interval(date_1, date_2):\n",
    "    return {\"create_ts\": date_1 + ' - ' + date_2}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'create_ts': '09/01/19 - 04/06/20'},\n",
       " {'political_file_type': 'PA'},\n",
       " {'office_type': 'Presidential'},\n",
       " {'campaign_year': '2020'},\n",
       " {'source_service_code': 'TV'},\n",
       " {'nielsen_dma_rank': 'BANGOR'},\n",
       " {'nielsen_dma_rank': 'BIRMINGHAM (ANN TUSC)'},\n",
       " {'nielsen_dma_rank': 'BOSTON (MANCHESTER)'},\n",
       " {'nielsen_dma_rank': 'BURLINGTON-PLATTSBURGH'},\n",
       " {'nielsen_dma_rank': 'CHARLOTTE'}]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Filters for OPIF File Search for Super Tuesday states in February\n",
    "\n",
    "filter_list = [\n",
    "            opif_timestamp_to_present('09/01/19'),\n",
    "            {\"political_file_type\":\"PA\"}, \n",
    "            {\"office_type\":\"Presidential\"},\n",
    "            {\"campaign_year\":\"2020\"},\n",
    "            {\"source_service_code\":\"TV\"}\n",
    "                      ]\n",
    "for dma in super_tuesday_dma: \n",
    "    filter_list.append({\"nielsen_dma_rank\":dma})\n",
    "filter_list[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Filters for OPIF File Search for all states in February\n",
    "# filter_list = [\n",
    "#             opif_timestamp_interval('02/01/20','02/29/20'),\n",
    "#             {\"political_file_type\":\"PA\"}, \n",
    "#             {\"office_type\":\"Presidential\"},\n",
    "#             {\"campaign_year\":\"2020\"},\n",
    "#             {\"source_service_code\":\"TV\"},\n",
    "#                       ] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Returns top ten results of OPIF File Search based on \n",
    "## keyword, filters, order, and offset.\n",
    "def opif_pub_search10(keyword, offset):\n",
    "    url = \"https://www.fcc.gov/search/api?t=opif\"\n",
    "    order = 'old'\n",
    "    filters = json.dumps(filter_list)\n",
    "    \n",
    "    # add parameters\n",
    "    parameters = {  'q': keyword,\n",
    "                    's': str(offset),\n",
    "                    'o': order,\n",
    "                    'f': {filters} }\n",
    "    response = requests.get(url, params = parameters)\n",
    "            \n",
    "    return response.json()['response']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mike Bloomberg 2020 Inc- 1409313\n",
      "Michael Bloomberg NAB\n",
      "Mike Bloomberg NAB 2020\n",
      "Mike Bloomberg 986337\n",
      "Bloomberg Spot Approval 1 11-22-19\n",
      "NAB Form Bloomberg US President 2020 - WWBT 11-22-19\n",
      "NAB Form Bloomberg US President 2020 - WUPV 11-22-19\n",
      "KGTV Bloomberg (Assembly) - Public File Disclosure\n",
      "11.21 NAB Form_LOCAL.pdf Michael Bloomberg\n",
      "Bloomberg 11_25 WWBT 1409383\n",
      "\n",
      "\n",
      "Michael Bloomberg NAB\n",
      "Mike Bloomberg NAB 2020\n",
      "Mike Bloomberg 986337\n",
      "Bloomberg Spot Approval 1 11-22-19\n",
      "NAB Form Bloomberg US President 2020 - WWBT 11-22-19\n",
      "NAB Form Bloomberg US President 2020 - WUPV 11-22-19\n",
      "KGTV Bloomberg (Assembly) - Public File Disclosure\n",
      "11.21 NAB Form_LOCAL.pdf Michael Bloomberg\n",
      "Bloomberg 11_25 WWBT 1409383\n",
      "Bloomberg 11_25 WUPV 1409539\n"
     ]
    }
   ],
   "source": [
    "## check for if offset is functioning\n",
    "r1 = opif_pub_search10('bloomberg', 1)\n",
    "r2 = opif_pub_search10('bloomberg', 2)\n",
    "for i in r1['docs']:\n",
    "    print (i['file_name'])\n",
    "print ('\\n')\n",
    "for i in r2['docs']:\n",
    "    print (i['file_name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "821"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "int(r1['numFound']/10) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from IPython.display import clear_output\n",
    "\n",
    "## Returns all results of OPIF File Search based on keyword.\n",
    "def opif_pub_search(keyword):\n",
    "    url = \"https://www.fcc.gov/search/api?t=opif\"\n",
    "    result = []\n",
    "    # initialize parameters\n",
    "    offset = 0\n",
    "    page = 1      # page = (offset/10) + 1\n",
    "    order = 'old' # choose from 'old', 'best', or 'new'\n",
    "    filters = json.dumps(filter_list)\n",
    "    \n",
    "    parameters = {  'q': keyword,\n",
    "                    's': str(offset),\n",
    "                    'o': order,\n",
    "                    'f': {filters} }\n",
    "    \n",
    "    # total_pages = (numFound / 10) + 1\n",
    "    total_pages = int(requests.get(url, params = parameters).json()['response']['numFound'] / 10) + 1\n",
    "    \n",
    "    while page <= total_pages:\n",
    "        parameters['s'] = str(offset)\n",
    "        \n",
    "        # prints status of method\n",
    "        print(\"Requesting page {}/{}\".format(page,total_pages))\n",
    "        clear_output(wait = True)\n",
    "        \n",
    "        # API call\n",
    "        response = requests.get(url, params = parameters)\n",
    "        \n",
    "        # if we get an error, print the response and halt the loop\n",
    "        if response.status_code != 200:\n",
    "            print(response.text)\n",
    "            break\n",
    "        \n",
    "        # if item index does not align with page, notify user and halt the loop\n",
    "        if response.json()['response']['start'] != offset:\n",
    "            print(\"Offset error at s = \" + str(offset))\n",
    "            break\n",
    "        \n",
    "        # add to our result set\n",
    "        result.append(response)\n",
    "        \n",
    "        # if not a cached result, sleep\n",
    "        if not getattr(response, 'from_cache', False):\n",
    "            time.sleep(0.25)\n",
    "        \n",
    "        offset += 10\n",
    "        page = int(offset/10) + 1\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Converts result set to DataFrame, drops duplicates, and saves as pickle\n",
    "def save_as_df(result, candidate):\n",
    "    frames = [pd.DataFrame(r.json()['response']['docs']) for r in result]\n",
    "    df = pd.concat(frames, sort=True)\n",
    "    print('Number of duplicates dropped: ', df.duplicated(subset = 'id', keep = 'first').sum())\n",
    "    df.drop_duplicates(subset = 'id', keep = 'first', inplace = True)\n",
    "    df.reset_index(drop = True, inplace = True)\n",
    "    \n",
    "    ## Inserts URL link as stem + folder_id + '/' + file_manager_id + '.' + file_extension\n",
    "    file_url = 'https://publicfiles.fcc.gov/api/manager/download/' + df['folder_id'] + '/' + df['file_manager_id'] + '.' + df['file_extension']\n",
    "    \n",
    "    df.insert(3, 'file_url', file_url)\n",
    "    df_name = str(candidate) + '.pkl'\n",
    "    df.to_pickle(df_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run each search at least twice in case you run into a 500 HTTP status code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requesting page 821/821\n"
     ]
    }
   ],
   "source": [
    "bloomberg_files = opif_pub_search('mike') + opif_pub_search('michael') + opif_pub_search('bloomberg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of duplicates dropped:  7082\n"
     ]
    }
   ],
   "source": [
    "save_as_df(bloomberg_files, 'bloomberg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requesting page 170/170\n"
     ]
    }
   ],
   "source": [
    "sanders_files = opif_pub_search('sanders') + opif_pub_search('bernie')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of duplicates dropped:  1419\n"
     ]
    }
   ],
   "source": [
    "save_as_df(sanders_files, 'sanders')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requesting page 30/30\n"
     ]
    }
   ],
   "source": [
    "biden_files = opif_pub_search('biden') + opif_pub_search('joe')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of duplicates dropped:  292\n"
     ]
    }
   ],
   "source": [
    "save_as_df(biden_files, 'biden')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requesting page 24/24\n"
     ]
    }
   ],
   "source": [
    "warren_files = opif_pub_search('warren') + opif_pub_search('elizabeth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of duplicates dropped:  238\n"
     ]
    }
   ],
   "source": [
    "save_as_df(warren_files, 'warren')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requesting page 48/48\n"
     ]
    }
   ],
   "source": [
    "buttigieg_files = opif_pub_search('buttigieg') + opif_pub_search('pete')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of duplicates dropped:  353\n"
     ]
    }
   ],
   "source": [
    "save_as_df(buttigieg_files, 'buttigieg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requesting page 69/69\n"
     ]
    }
   ],
   "source": [
    "klobuchar_files = opif_pub_search('amy') + opif_pub_search('klobuchar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of duplicates dropped:  545\n"
     ]
    }
   ],
   "source": [
    "save_as_df(klobuchar_files, 'klobuchar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requesting page 265/265\n"
     ]
    }
   ],
   "source": [
    "steyer_files = opif_pub_search('tom') + opif_pub_search('steyer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of duplicates dropped:  2136\n"
     ]
    }
   ],
   "source": [
    "save_as_df(steyer_files, 'steyer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
