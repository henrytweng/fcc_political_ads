{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_pickle(\"OPIF_04-06.pkl\")\n",
    "select = 'KRWF'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "WHNS       229\n",
       "WMUR-TV    211\n",
       "KRWF       180\n",
       "KSAX       180\n",
       "KFMB-TV    169\n",
       "KTTC       159\n",
       "WPTZ       156\n",
       "KBJR-TV    152\n",
       "KRII       152\n",
       "KMPH-TV    147\n",
       "Name: callsign, dtype: int64"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['callsign'].value_counts()[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfquery = df[(df.callsign == select)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully created the directory KRWF Files/ \n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "path = select + ' Files/'\n",
    "try:\n",
    "    os.mkdir(path)\n",
    "except OSError:\n",
    "    print (\"Creation of the directory %s failed\" % path)\n",
    "else:\n",
    "    print (\"Successfully created the directory %s \" % path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests, requests_cache, time\n",
    "from IPython.display import clear_output\n",
    "requests_cache.install_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfquery.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of errors:  5\n"
     ]
    }
   ],
   "source": [
    "n = 0\n",
    "errorCount = 0\n",
    "for url, name in zip(dfquery.file_url, dfquery.file_name):\n",
    "    print(\"Downloading file {}/{}\".format(n+1,len(dfquery)))\n",
    "    \n",
    "    r = requests.get(url, stream = True)\n",
    "    if (r.status_code != 200): \n",
    "        print(url, r.text)\n",
    "        errorCount += 1\n",
    "        time.sleep(2)\n",
    "    else: \n",
    "        with open(path + name + '.pdf', 'wb') as f:\n",
    "            f.write(r.content)\n",
    "        dfquery.loc[n,'file_path'] = path + name + '.pdf'\n",
    "    if not getattr(r, 'from_cache', False):\n",
    "        time.sleep(0.25)\n",
    "    clear_output(wait = True)\n",
    "    n += 1\n",
    "print(\"Number of errors: \", errorCount)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\Henry Weng\\\\PythonNotebooks\\\\KSAX Files.zip'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import shutil\n",
    "# shutil.make_archive(select + ' Files', 'zip', select + ' Files')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scrapes all the PDFs and appends resulting strings as a dataframe column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from io import StringIO\n",
    "from pdfminer.converter import TextConverter\n",
    "from pdfminer.layout import LAParams\n",
    "from pdfminer.pdfdocument import PDFDocument\n",
    "from pdfminer.pdfinterp import PDFResourceManager, PDFPageInterpreter\n",
    "from pdfminer.pdfpage import PDFPage\n",
    "from pdfminer.pdfparser import PDFParser\n",
    "\n",
    "def scrape(pdf):\n",
    "    output_string = StringIO()\n",
    "    with open(pdf, 'rb') as in_file:\n",
    "        parser = PDFParser(in_file)\n",
    "        doc = PDFDocument(parser)\n",
    "        rsrcmgr = PDFResourceManager()\n",
    "        device = TextConverter(rsrcmgr, output_string, laparams=LAParams())\n",
    "        interpreter = PDFPageInterpreter(rsrcmgr, device)\n",
    "        for page in PDFPage.create_pages(doc):\n",
    "            interpreter.process_page(page)\n",
    "    return output_string.getvalue()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfquery = dfquery[dfquery['file_path'].notnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "dfquery['scrape_output'] = dfquery['file_path'].apply(scrape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dfquery.to_pickle('dfquery.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Resuming session\n",
    "import pandas as pd\n",
    "dfq = pd.read_pickle('dfquery.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfq.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True     110\n",
       "False     65\n",
       "Name: scrape_output, dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfq['scrape_output'].str.contains('Contract Date').value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False    110\n",
       "True      65\n",
       "Name: scrape_output, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfq['scrape_output'].str.contains('Flight Date').value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We want to find the search parameter for flight/contract dates and validate that they work for each file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['03/03/20 - 03/03/20\\n\\nAd',\n",
       " '\\n\\n03/03/20 - 03/03/20\\n\\n',\n",
       " '03/02/20 - 03/03/20\\n\\nAd',\n",
       " '\\n\\n03/02/20 - 03/03/20\\n\\n',\n",
       " '\\n\\n02/26/20 - 03/03/20\\n\\n']"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f_dates = []\n",
    "for row in dfq[dfq['scrape_output'].str.contains('Flight Date')]['scrape_output']:\n",
    "    start = row.find('Flight Date') + 14\n",
    "    end = start + 23\n",
    "    f_dates.append(row[start:end])\n",
    "f_dates[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['\\n\\nEstimate #\\n\\n03/02/20 - 03/03/20\\n\\n7173\\n\\nAdvertise',\n",
       " '\\n\\nEstimate #\\n\\n03/03/20 - 03/03/20\\n\\n7173\\n\\nAdvertise',\n",
       " '\\n\\nEstimate #\\n\\n03/02/20 - 03/03/20\\n\\n7173\\n\\nAdvertise',\n",
       " '\\n\\nEstimate #\\n\\n02/26/20 - 03/03/20\\n\\n69\\n\\nAdvertiser\\n',\n",
       " '\\n\\nEstimate #\\n\\n02/26/20 - 03/03/20\\n\\n69\\n\\nAdvertiser\\n']"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c_dates = []\n",
    "for row in dfq[dfq['scrape_output'].str.contains('Contract Date')]['scrape_output']:\n",
    "    start = row.find('Contract Date') + 14\n",
    "    end = start + 50\n",
    "    c_dates.append(row[start:end])\n",
    "c_dates[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'03/02/20 - 03/03/20'"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "dateRegex = re.compile(r'((\\d{2})/(\\d{2})/(\\d{2}) - (\\d{2})/(\\d{2})/(\\d{2}))')\n",
    "m = re.search(dateRegex, c_dates[0])\n",
    "m.group()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'03/03/20 - 03/03/20'"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n = re.search(dateRegex, f_dates[0])\n",
    "n.group()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now, we generalize to all rows. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "dfq['f_start'] = np.where(dfq['scrape_output'].str.contains('Flight Date'),\n",
    "                             dfq['scrape_output'].str.find('Flight Date') + 14, 0)\n",
    "dfq['f_end'] = np.where(dfq['scrape_output'].str.contains('Flight Date'),\n",
    "                             dfq['scrape_output'].str.find('Flight Date') + 37, 0)\n",
    "dfq['c_start'] = np.where(dfq['scrape_output'].str.contains('Contract Date'),\n",
    "                             dfq['scrape_output'].str.find('Contract Date') + 14, 0)\n",
    "dfq['c_end'] = np.where(dfq['scrape_output'].str.contains('Contract Date'),\n",
    "                             dfq['scrape_output'].str.find('Contract Date') + 50, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Grabs the desired section from each scrape_output.\n",
    "f_dates = pd.Series([A[B:C] for A,B,C in zip(dfq.scrape_output, dfq.f_start, dfq.f_end)])\n",
    "c_dates = pd.Series([A[B:C] for A,B,C in zip(dfq.scrape_output, dfq.c_start, dfq.c_end)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# flight_dates = D.replace(to_replace=r'[a-zA-Z\\n]',value='',regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfq['Flight_Date'] = f_dates.str.extract(dateRegex)[0]\n",
    "dfq['Contract_Date'] = c_dates.str.extract(dateRegex)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gets the highest dollar amount."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getTotal(in_string):\n",
    "    amtRegex = re.compile(r'(\\$)(\\d{1,3})(,\\d{3})?(.\\d{2})?')\n",
    "    mo = re.findall(amtRegex, in_string)\n",
    "    numbers = []\n",
    "    for match in mo:\n",
    "        entry = ''\n",
    "        for j in match:\n",
    "            if(j != '$'): entry += j\n",
    "        numbers.append(entry)\n",
    "    for i in range(len(numbers)):\n",
    "        numbers[i] = numbers[i].replace(',', '')\n",
    "    num_array = list(map(float, numbers))\n",
    "    if not num_array: \n",
    "        return 0\n",
    "    return max(num_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfq['Amount'] = dfq['scrape_output'].apply(getTotal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        4230.00\n",
       "1         465.00\n",
       "2        4230.00\n",
       "3         465.00\n",
       "4         465.00\n",
       "         ...    \n",
       "170     24775.00\n",
       "171     24775.00\n",
       "172     23565.00\n",
       "173    130394.74\n",
       "174      1210.00\n",
       "Name: Amount, Length: 175, dtype: float64"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfq['Amount']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Attempts to get the total spots field."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getNumberSpots(in_string):\n",
    "    if(in_string.count('Total Spots') > 0):\n",
    "        start = in_string.rfind('Total Spots') + 11\n",
    "        end = start + 15  # originally 5\n",
    "        return in_string[start:end]\n",
    "    elif(in_string.count('Totals') > 0):\n",
    "        start = in_string.rfind('Totals') + 6\n",
    "        end = start + 15 # originally 6\n",
    "        return in_string[start:end]\n",
    "    return 'N/A'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    \\n\\n24\\n\\n24\\n\\n$4,23\n",
       "1    \\n\\n24\\n\\n$465.00\\n\\n\n",
       "2    \\n\\n25\\n\\n25\\n\\n$4,23\n",
       "3      \\n\\n24\\n\\n8 03/03/2\n",
       "4      \\n\\n24\\n\\n$465.00\\n\n",
       "5      \\n\\n25\\n\\nPayment T\n",
       "6       \\n\\n25\\n\\n$4,230.0\n",
       "7    \\n\\n45\\n\\n45\\n\\n$23,2\n",
       "8      \\n\\n133\\n\\n$51,695.\n",
       "9    \\n\\n97\\n\\n97\\n\\n$44,3\n",
       "Name: scrape_output, dtype: object"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nums = dfq['scrape_output'].apply(getNumberSpots)\n",
    "nums[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n24\\n\\n$'"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numRegex = re.compile(r'(\\n)(\\d{1,3})(\\n)(\\n)(\\$)')\n",
    "n = re.search(numRegex, nums[1])\n",
    "n.group()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generalizing for all rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfq['n_start'] = np.where(dfq['scrape_output'].str.contains('Totals'),\n",
    "                             dfq['scrape_output'].str.rfind('Totals') + 6, 0)\n",
    "dfq['n_end'] = np.where(dfq['scrape_output'].str.contains('Totals'),\n",
    "                             dfq['scrape_output'].str.rfind('Totals') + 21, 0)\n",
    "dfq['t_start'] = np.where(dfq['scrape_output'].str.contains('Total Spots'),\n",
    "                             dfq['scrape_output'].str.rfind('Total Spots') + 11, 0)\n",
    "dfq['t_end'] = np.where(dfq['scrape_output'].str.contains('Total Spots'),\n",
    "                             dfq['scrape_output'].str.rfind('Total Spots') + 26, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_spots = pd.Series([A[B:C] for A,B,C in zip(dfq.scrape_output, dfq.t_start, dfq.t_end)])\n",
    "totals = pd.Series([A[B:C] for A,B,C in zip(dfq.scrape_output, dfq.n_start, dfq.n_end)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfq['Total_Spots'] = total_spots.str.extract(numRegex)[1]\n",
    "dfq['Totals'] = totals.str.extract(numRegex)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      NaN\n",
       "1      NaN\n",
       "2      NaN\n",
       "3      NaN\n",
       "4      NaN\n",
       "      ... \n",
       "170    NaN\n",
       "171    NaN\n",
       "172    NaN\n",
       "173    NaN\n",
       "174    NaN\n",
       "Name: Total_Spots, Length: 175, dtype: object"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfq['Total_Spots']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Export to Excel\n",
    "dfexp = dfq[['candidate','date','callsign','file_name',\n",
    "             'Flight_Date','Contract_Date','Amount','Totals','Total_Spots','file_url']]\n",
    "dfexp.to_excel(select + '-04-06.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
